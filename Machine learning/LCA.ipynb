{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.ndimage\n",
    "from scipy.signal import chirp, find_peaks, peak_widths, peak_prominences\n",
    "from scipy.signal import chirp, find_peaks, peak_widths, peak_prominences\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from EDA_Features2 import *\n",
    "from TEMP import *\n",
    "from ECG_features2 import * \n",
    "from ECG_features3 import * \n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "print(\"Start!\")\n",
    "\n",
    "data_set_path = \"D:/Downloads/WESAD/WESAD/\"\n",
    "subject = [\"S2\",'S3', 'S4', 'S5', 'S6', 'S7', 'S8', 'S9', 'S10', 'S11', 'S13', 'S14', 'S15', 'S16', 'S17']\n",
    "\n",
    "features_base = np.asarray(np.zeros(36), dtype = \"float\")\n",
    "features_stress = np.asarray(np.zeros(36), dtype = \"float\")\n",
    "\n",
    "#######################################################################\n",
    "#Reading out subjects and calling the feature extraction functions\n",
    "for i in range(len(subject)):     \n",
    "    print(\"subject: \", subject[i])\n",
    "\n",
    "    obj_data = {}\n",
    "\n",
    "    obj_data[subject[i]] = read_data_of_one_subject(data_set_path, subject[i])\n",
    "    #print(obj_data[subject[i]].data)\n",
    "    chest_data_dict = obj_data[subject[i]].get_chest_data()\n",
    "\n",
    "    labels = obj_data[subject[i]].get_labels() \n",
    "    baseline = np.asarray([idx for idx,val in enumerate(labels) if val == 1])\n",
    "    stress = np.asarray([idx for idx,val in enumerate(labels) if val == 2])\n",
    "\n",
    "    eda_data_stress=chest_data_dict['EDA'][stress,0]\n",
    "    eda_data_base=chest_data_dict['EDA'][baseline,0]\n",
    "\n",
    "    temp_data_stress=chest_data_dict['Temp'][stress,0]\n",
    "    temp_data_base=chest_data_dict['Temp'][baseline,0]\n",
    "\n",
    "    ecg_data_stress=chest_data_dict['ECG'][stress,0]\n",
    "    ecg_data_base=chest_data_dict['ECG'][baseline,0]    \n",
    "\n",
    "    eda_features_base = calc_eda_features(eda_data_base)\n",
    "    eda_features_stress = calc_eda_features(eda_data_stress)\n",
    "\n",
    "    temp_features_base = calc_temp_features(temp_data_base)\n",
    "    temp_features_stress = calc_temp_features(temp_data_stress)\n",
    "\n",
    "    ecg_features_time_base = ECG_time_data(ecg_data_base)\n",
    "    ecg_features_time_stress = ECG_time_data(ecg_data_stress)\n",
    "\n",
    "    ecg_features_freq_base = ECG_freq_data(ecg_data_base)\n",
    "    ecg_features_freq_stress = ECG_freq_data(ecg_data_stress)\n",
    "\n",
    "    #print(ecg_features_freq_base)\n",
    "    #print(ecg_features_freq_base.shape)\n",
    "\n",
    "    #print(ecg_features_freq_stress)\n",
    "    #print(ecg_features_freq_stress.shape)\n",
    "\n",
    "    np.reshape(eda_features_stress, (1,-1))\n",
    "\n",
    "    #print(eda_features_stress.shape, temp_features_stress.shape,ecg_features_time_stress.shape, ecg_features_freq_stress.shape)\n",
    "\n",
    "    features_stress = np.vstack((features_stress, np.hstack((eda_features_stress, temp_features_stress, ecg_features_time_stress, ecg_features_freq_stress))  ))\n",
    "    features_base = np.vstack((features_base, np.hstack((eda_features_base, temp_features_base, ecg_features_time_base, ecg_features_freq_base)) ))\n",
    "\n",
    "features_base = features_base[1:,:]\n",
    "features_stress = features_stress[1:,:]\n",
    "#print(\"feat_base:\")\n",
    "#print(features_base)\n",
    "#print(features_base.shape)\n",
    "#print(\"feat_stress:\")\n",
    "#print(features_stress)\n",
    "#print(features_stress.shape)\n",
    "\n",
    "features_in = np.vstack((features_base,features_stress))\n",
    "#print(\"feat_in:\")\n",
    "#print(features_in)\n",
    "#print(features_in.shape)\n",
    "stress_state = np.append( np.zeros(features_base.shape[0]) , np.ones(features_stress.shape[0]) )\n",
    "#print(\"stress_state:\")\n",
    "#print(stress_state)\n",
    "#print(stress_state.shape)\n",
    "#stress_state = np.ravel(stress_state)\n",
    "\n",
    "\n",
    "########################################################################\n",
    "#LDA\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_in, stress_state, test_size=0.25, random_state=42)\n",
    "\n",
    "lda=LDA(n_components=1)\n",
    "train_lda=lda.fit(X_train, y_train)\n",
    "test_lda=lda.predict(X_test)\n",
    "\n",
    "# print(test_lda.shape)\n",
    "# print(y_test.shape)\n",
    "\n",
    "score= lda.score(X_test,y_test)\n",
    "print('Score:', score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "######################################################################\n",
    "#Neural Network\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import keras as keras\n",
    "\n",
    "# DNN = Sequential()\n",
    "\n",
    "# # The Input Layer :\n",
    "# DNN.add(Dense(128, kernel_initializer='normal',input_dim = X_train.shape[1], activation='relu'))\n",
    "\n",
    "# # The Hidden Layers :\n",
    "# DNN.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "# DNN.add(Dropout(0.25))  \n",
    "# DNN.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "# DNN.add(Dropout(0.25))\n",
    "# DNN.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "# DNN.add(Dropout(0.25))\n",
    "\n",
    "# adding layers\n",
    "\n",
    "\n",
    "\n",
    "# The Output Layer :\n",
    "# DNN.add(Dense(1, kernel_initializer='normal',activation='linear'))\n",
    "\n",
    "# # Compile the network :\n",
    "# DNN.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# DNN.summary()\n",
    "\n",
    "\n",
    "# # checkpoint\n",
    "# '''filepath=\"weights-improvement-{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
    "# checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='max')\n",
    "# callbacks_list = [checkpoint]'''\n",
    "\n",
    "# checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
    "# checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
    "# callbacks_list = [checkpoint]\n",
    "\n",
    "#  #Fit the model\n",
    "# history = DNN.fit(X_train,y_train,validation_data=(X_test,y_test), epochs=300, batch_size=16, verbose=0, callbacks=callbacks_list)\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['val_loss'])\n",
    "\n",
    "# plt.plot(history.history['accuracy'])\n",
    "# plt.plot(history.history['val_accuracy'])\n",
    "\n",
    "# print(\"best accuracy:\", np.max(history.history['accuracy']))\n",
    "# print(\"best val accuracy\", np.max(history.history['val_accuracy']))\n",
    "\n",
    "\n",
    "# Create simple Neural Network model\n",
    "input_nodes = X_train.shape[1]\n",
    "hidden_layer_1_nodes = 20\n",
    "hidden_layer_2_nodes = 10\n",
    "output_layer = 1\n",
    "\n",
    "# initializing a sequential model\n",
    "full_model = Sequential()\n",
    "\n",
    "# adding layers\n",
    "full_model.add(Dense(hidden_layer_1_nodes,input_dim=input_nodes , activation='relu'))\n",
    "full_model.add(Dropout(0.1))\n",
    "full_model.add(Dense(hidden_layer_2_nodes, activation='relu'))\n",
    "full_model.add(Dropout(0.1))\n",
    "full_model.add(Dense(output_layer, activation='sigmoid'))\n",
    "\n",
    "full_model.summary()\n",
    "\n",
    "# Compiling the ANN\n",
    "full_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = full_model.fit(X_train,y_train,validation_data=(X_test,y_test), epochs=500, batch_size=32, verbose=2)\t\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "print(\"best accuracy:\",np.max(history.history['accuracy']))\n",
    "print(\"best val-accuracy:\", np.max(history.history['val_accuracy']))\n",
    "\n",
    "#######################################################################\n",
    "## K Cross fold validation\n",
    "\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "\n",
    "# prepare the cross-validation procedure\n",
    "cv = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "# evaluate model\n",
    "scores = cross_val_score(lda, features_in, stress_state, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# report performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))\n",
    "print(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
